{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9214dc7-bdb4-4434-9355-e4ac271793bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.0.0+cu117\n",
      "Torchvision Version:  0.15.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b0a0cd-f03f-4eba-acfe-1f33c21baff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1a427d2-759e-49e6-b0a5-e8cbe8d450c2",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a0a405-9fa7-4d3c-b842-b2d891175a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, progress, criterion, optimizer, num_epochs=25):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            progress[phase].reset()\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                progress[phase].update()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss_train = epoch_loss\n",
    "                acc_train = epoch_acc\n",
    "            if phase == 'val':\n",
    "                loss_valid = epoch_loss\n",
    "                acc_valid = epoch_acc\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        print()\n",
    "        # Log the loss and accuracy values at the end of each epoch\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": loss_train,\n",
    "            \"Train Acc\": acc_train,\n",
    "            \"Valid Loss\": loss_valid,\n",
    "            \"Valid Acc\": acc_valid})      \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inception module for model\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, output_1x1, output_5x5, output_3x3, output_pool):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.branch1x1 = nn.Conv2d(in_channels, output_1x1, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, output_5x5[0], kernel_size=1),\n",
    "            nn.Conv2d(output_5x5[0], output_5x5[1], kernel_size=5, padding=2)\n",
    "        )\n",
    "        \n",
    "        self.branch3x3dbl = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(96, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch3x3dbl = self.branch3x3dbl(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "        \n",
    "        out = torch.cat((branch1x1, branch5x5, branch3x3dbl, branch_pool), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a35548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inception model class\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes, is_training=True, dropout_keep_prob=0.8):\n",
    "        super(InceptionV3, self).__init__()\n",
    "        \n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.conv0 = nn.Conv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 80, kernel_size=1)\n",
    "        self.conv4 = nn.Conv2d(80, 192, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        # Inception blocks\n",
    "        self.inception1 = InceptionBlock(192)\n",
    "        # self.inception2 = InceptionBlock(256)\n",
    "        # self.inception3 = InceptionBlock(288)\n",
    "        # add more !!\n",
    "\n",
    "        # Auxiliary Head logits\n",
    "        self.aux_logits = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=5, stride=3),\n",
    "            nn.Conv2d(288, 128, kernel_size=1),\n",
    "            nn.Conv2d(128, 768, kernel_size=shape[1:3]),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(768, num_classes)\n",
    "        )\n",
    "        # Final pooling and prediction\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=shape[1:3]),\n",
    "            nn.Dropout(dropout_keep_prob),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Option 1 (no inception blocks yet)\n",
    "        # conv0 = self.conv0(inputs)\n",
    "        # conv1 = self.conv1(conv0)\n",
    "        # conv2 = self.conv2(conv1)\n",
    "        # pool1 = self.pool1(conv2)\n",
    "        # conv3 = self.conv3(pool1)\n",
    "        # conv4 = self.conv4(conv3)\n",
    "        # pool2 = self.pool2(conv4)\n",
    "\n",
    "        # Option 2\n",
    "        x = F.relu(self.conv0(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Apply Inception blocks\n",
    "        x = self.inception1(x)\n",
    "        # x = self.inception2(x)\n",
    "        # x = self.inception3(x)\n",
    "\n",
    "        aux_logits = self.aux_logits(x)\n",
    "        logits = self.logits(x)\n",
    "\n",
    "        if self.is_training:\n",
    "            if self.restore_logits:\n",
    "                predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "                print(\"softmax training\")\n",
    "            else:\n",
    "                predictions = torch.nn.functional.sigmoid(logits)\n",
    "                print(\"sigmoid training\")\n",
    "        else:\n",
    "            predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "            print(\"softmax default training\")\n",
    "\n",
    "        return logits, predictions\n",
    "\n",
    "        # return pool2\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceef88a9-04c6-4ab4-9c73-d35a3ee7f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(num_classes):\n",
    "    # Define the model architecture\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e015f906-1545-4b6e-b1b9-68102312948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split image folders into train, val, test\n",
    "def split_data(patch_directory, split: list, seed):\n",
    "    '''\n",
    "    Function that takes in the split percentage for train/val/test sets, and randomly chooses which cases\n",
    "    to allocate to which set (to ensure all patches from one case go into one set)\n",
    "    Parameters:\n",
    "    patch_directory: folder containing all patches\n",
    "    split: list of integers for splitting sets\n",
    "    seed: option to set the seed value for randomness\n",
    "    Returns:\n",
    "    3 lists for each of train/val/test, where each list contains the case names to be used in the set\n",
    "    '''\n",
    "    \n",
    "    random.seed(seed)\n",
    "\n",
    "    case_folders = os.listdir(patch_directory) # get 147 case folders\n",
    "    \n",
    "    d = {}\n",
    "    for folder in case_folders:\n",
    "        num_patches_in_folder = len(os.listdir(patch_directory + folder))\n",
    "        d[folder] = num_patches_in_folder\n",
    "    \n",
    "    total_num_patches = sum(d.values())\n",
    "    train_split, val_split, test_split = split\n",
    "    train_num_patches = int((train_split/100)*total_num_patches)\n",
    "    val_num_patches = int((val_split/100)*total_num_patches)\n",
    "\n",
    "    # list all folders in the directory\n",
    "    folders = [os.path.join(patch_directory, folder) for folder in os.listdir(patch_directory) if os.path.isdir(os.path.join(patch_directory, folder))]\n",
    "    \n",
    "    # SELECT TRAINING CASES\n",
    "    train_cases = [] # store all selected cases\n",
    "    num_selected_train = 0 # number of patches selected so far\n",
    "    selected_folders = set() # a set to store the selected folder names to keep track of those already selected\n",
    "    while num_selected_train < train_num_patches:\n",
    "        folder = random.choice(folders)\n",
    "        if folder not in selected_folders:\n",
    "            case = folder.replace(patch_directory, '')\n",
    "            num_patches = len(os.listdir(folder))\n",
    "            num_selected_train += num_patches\n",
    "            selected_folders.add(folder) # add to set of selected folders\n",
    "            train_cases.append(case)\n",
    "\n",
    "    # SELECT VAL CASES\n",
    "    val_cases = [] # store all selected cases\n",
    "    num_selected_val = 0 # number of patches selected so far\n",
    "    while num_selected_val < val_num_patches:\n",
    "        folder = random.choice(folders)\n",
    "        if folder not in selected_folders:\n",
    "            case = folder.replace(patch_directory, '')\n",
    "            num_patches = len(os.listdir(folder))\n",
    "            num_selected_val += num_patches\n",
    "            selected_folders.add(folder)\n",
    "            val_cases.append(case)\n",
    "\n",
    "    # SELECT TEST CASES\n",
    "    cases = [folder.replace(patch_directory, '') for folder in folders]\n",
    "    used = train_cases+val_cases\n",
    "    test_cases = [case for case in cases if case not in used]\n",
    "    \n",
    "    # test_patches = [len(os.listdir(patch_directory + folder)) for folder in test_cases]\n",
    "    num_selected_test = sum([len(os.listdir(patch_directory + folder)) for folder in test_cases])\n",
    "    # dict = {x: for x in ['train', 'val', 'test']}\n",
    "    print(f\"Number of training patches: {num_selected_train} \\nNumber of validation patches {num_selected_val} \\nNumber of test patches {num_selected_test}\")\n",
    "    return train_cases, val_cases, test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee90016-f684-4953-933a-bf5d4af70332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch dataset to read in your images and apply transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_folders, label_files, transform=None):\n",
    "        self.img_folders = img_folders\n",
    "        self.label_files = label_files\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs = [] # Keeps image paths to load in the __getitem__ method\n",
    "        self.labels = []\n",
    "\n",
    "        # Load images and corresponding labels\n",
    "        for i, (img_folder, label_file) in enumerate(zip(img_folders, label_files)):\n",
    "            # print(\"Patch directory\", img_folder, \"\\nLabel file\", label_file)\n",
    "            labels_pt = torch.load(label_file) # Load .pt file\n",
    "            # Run through all patches from the case folder\n",
    "            for i, img in enumerate(os.listdir(img_folder)):\n",
    "                if os.path.isfile(img_folder + '/' + img) and os.path.isfile(label_file):\n",
    "                    # print(img_folder + img)\n",
    "                    if img.startswith('._'):\n",
    "                        img = img.replace('._', '')\n",
    "                    idx = int(img.replace('.png', '').split(\"_\")[1])\n",
    "                    self.imgs.append(img_folder + '/' + img)\n",
    "                    self.labels.append(labels_pt[idx].item()) # get label as int\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image at given index\n",
    "        image_path = self.imgs[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None: # Apply transformations\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx] # Load corresponding image label\n",
    "        \n",
    "        return image, label # Return transformed image and label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c80fee-5d59-41da-bc97-72467d0e66b0",
   "metadata": {},
   "source": [
    "### Initialise simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1d2149-f39b-430a-8085-f31818522356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=65536, out_features=512, bias=True)\n",
      "  (11): ReLU()\n",
      "  (12): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "CNN_model = initialise_model(num_classes)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(CNN_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea94bc4-8bbf-4cf7-9dfc-c670378322e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(CNN_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d9429f-ed91-4c1e-b610-f0291554e90e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ef1868-b12b-418b-82a1-606d6cb28176",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE=256\n",
    "STRIDE=PATCH_SIZE\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdb15c8-becc-43c0-ade3-40f51d35b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise data transforms\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.Resize(INPUT_SIZE),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Initially no colour normalisation\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        # transforms.Resize(INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test' : transforms.Compose([\n",
    "        # transforms.Resize(INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c51bf8-62bb-43de-a01d-b21d34b848d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training patches: 258108 \n",
      "Number of validation patches 58768 \n",
      "Number of test patches 47709\n"
     ]
    }
   ],
   "source": [
    "# using full set of data\n",
    "\n",
    "img_dir = '../data/patches/'\n",
    "labels_dir = '../data/labels/'\n",
    "\n",
    "split=[70, 15, 15] # for splitting into train/val/test\n",
    "\n",
    "train_cases, val_cases, test_cases = split_data(img_dir, split, SEED)\n",
    "\n",
    "train_img_folders = [img_dir + case for case in train_cases]\n",
    "val_img_folders = [img_dir + case for case in val_cases]\n",
    "test_img_folders = [img_dir + case for case in test_cases]\n",
    "\n",
    "# Contains the file path for each .pt file for the cases used in each of the sets\n",
    "train_labels = [labels_dir + case + '.pt' for case in train_cases]\n",
    "val_labels = [labels_dir + case + '.pt' for case in val_cases]\n",
    "test_labels = [labels_dir + case + '.pt' for case in test_cases]\n",
    "\n",
    "image_datasets = {\n",
    "    'train': CustomDataset(train_img_folders, train_labels, transform=data_transforms['train']),\n",
    "    'val': CustomDataset(val_img_folders, val_labels, transform=data_transforms['val']),\n",
    "    'test': CustomDataset(test_img_folders, test_labels, transform=data_transforms['test'])\n",
    "}\n",
    "# Create training, validation and test dataloaders\n",
    "dataloaders = {\n",
    "    'train': data_utils.DataLoader(image_datasets['train'], batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True),\n",
    "    'val': data_utils.DataLoader(image_datasets['val'], batch_size=batch_size, num_workers=4, shuffle=True),\n",
    "    'test': data_utils.DataLoader(image_datasets['test'], batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "}\n",
    "# num_workers=?, drop_last=True\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcb1b428-2639-47b0-bf30-07cb90b88f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Check format of data\n",
    "print(image_datasets['train'][0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7ea19-eec7-4804-850d-4814be2dc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b334a0e8-b3b7-4613-8940-08cc648f0d16",
   "metadata": {},
   "source": [
    "### Create optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0add70-b26a-400c-a884-ac5b9310196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a0c03-fdab-4028-84a6-ca9e977147a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "optimiser = optim.SGD(CNN_model.parameters(), lr=learning_rate, weight_decay=learning_rate_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc9d1cb-35c9-447c-a82a-b2b3100f6107",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e6361-e4aa-47cf-96be-ce9fa777abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_NOTEBOOK_NAME = 'simple_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667548a7-5b11-4a6a-b917-b9e3f6e74a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f817422-080e-497f-b683-9d91dd95f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB \n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"masters\",\n",
    "    notes=\"Practice run, simple 6-layer CNN\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": learning_rate_decay,\n",
    "        \"epochs\": num_epochs,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc44e6-11a3-4885-aeaf-3ffebeaf0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = {'train': tqdm(total=len(dataloaders['train']), desc=\"Training progress\"), 'val': tqdm(total=len(dataloaders['val']), desc=\"Validation progress\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b9234-017b-4371-af4e-288c6a656ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "CNN_model = train_model(CNN_model, dataloaders, progress, criterion, optimiser, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781029e-5c58-4088-9b34-941edb3591d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea2303-8031-44eb-8f90-581490d3f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4c0b5-bb62-453b-a447-b893c6abb1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on example image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
